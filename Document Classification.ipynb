{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <span style=\"color:#1E90FF\">\n",
    "    <h1>\n",
    "        Practical Web Mining for Business Applications\n",
    "    </h1>\n",
    "    <h2>\n",
    "        Document Classification\n",
    "    </h2>\n",
    "    </span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of path for all the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'E:\\corpus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center style=\"color:#1E90FF\">\n",
    "    <h1>\n",
    "        1. Text Pre-Processing and Exploration\n",
    "    </h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load the corpus and split it to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corupus loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as sk\n",
    "\n",
    "train_set = sk.load_files(path + \"\\\\training\", description=None, categories=None, \n",
    "                        load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
    "test_set = sk.load_files(path + \"\\\\test\", description=None, categories=None, \n",
    "                       load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
    "\n",
    "# I wish to load the train data again to display differences later on\n",
    "backup_train = sk.load_files(path + \"\\\\training\", description=None, categories=None, \n",
    "                        load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
    "\n",
    "print(\"Corupus loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and normalize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets define some global variables and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "term_per_category = defaultdict(lambda: defaultdict(int))\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop iterates the files clean it, by using: tokenization, lower case, stop words removal, stemming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will loop the text and count the distrebution of terms in each category into 'term_per_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    for i, target in zip(range(len(data.data)), data.target):\n",
    "        #------------TO LOWER CASE-------------\n",
    "        file = data.data[i].decode(\"utf-8\").lower()\n",
    "        #------------TOKENIZE-------------\n",
    "        word_tokens = word_tokenize(file)\n",
    "        #------------REMOVE STOP WORDS-------------\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "        #------------STEMMING WITH PORTER STEMMER-------------\n",
    "        ps = PorterStemmer()\n",
    "        stemmedFile = []\n",
    "        for word in filtered_sentence:\n",
    "            for w in word.split(\" \"):\n",
    "                stem = ps.stem(w)\n",
    "                stemmedFile.append(stem)\n",
    "                #COUNT THE TERMS PER CATEGORY\n",
    "                term_per_category[train_set.target_names[target]][word] += 1\n",
    "        #------------PUT FILE BACK-------------\n",
    "        data.data[i] = ' '.join(stemmedFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets use the methode above to clean the Train and the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_text(train_set)\n",
    "clean_text(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Differences between the train data before and after normalizing and cleaning the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Management of Cushing's syndrome secondary to adrenal adenoma during pregnancy.\\n This report discusses our experience with two patients who had unilateral adrenalectomy during pregnancy as treatment for Cushing's syndrome secondary to an adrenal adenoma.\\n Previously only five patients with this clinical problem who underwent unilateral adrenalectomy during pregnancy had been reported.\\n We have reviewed the world literature on Cushing's syndrome in pregnancy secondary to an adrenal adenoma.\\n A total of 19 patients who had unilateral adrenalectomy for this problem after the completion of pregnancy were identified.\\n The review of world literature and the two patients who are the subject of this report were the basis of our analysis of fetal death, neonatal complications, and maternal complications in seven pregnancies during which unilateral adrenalectomy was performed (group 1) compared to the 19 pregnancies that were associated with unilateral adrenalectomy at the completion of pregnancy (group 2).\\n Of the seven pregnancies in group 1, one fetal death and no neonatal complications occurred, but fetal death and neonatal complications occurred in 12 of the 19 pregnancies in group 2.\\n Four of the seven mothers in group 1 had complications; 16 of the 19 mothers in group 2 had complications.\\n This study suggests that adrenalectomy during pregnancy should be considered as a therapeutic option in the management of Cushing's syndrome secondary to an adrenal cortical adenoma.\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backup_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"manag cush 's syndrom secondari adren adenoma pregnanc report discuss experi two patient unilat adrenalectomi pregnanc treatment cush 's syndrom secondari adren adenoma previou five patient clinic problem underw unilat adrenalectomi pregnanc report review world literatur cush 's syndrom pregnanc secondari adren adenoma total 19 patient unilat adrenalectomi problem complet pregnanc identifi review world literatur two patient subject report basi analysi fetal death neonat complic matern complic seven pregnanc unilat adrenalectomi perform group 1 compar 19 pregnanc associ unilat adrenalectomi complet pregnanc group 2 seven pregnanc group 1 one fetal death neonat complic occur fetal death neonat complic occur 12 19 pregnanc group 2. four seven mother group 1 complic 16 19 mother group 2 complic studi suggest adrenalectomi pregnanc consid therapeut option manag cush 's syndrom secondari adren cortic adenoma\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the amount of categories, and their names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 categories\n",
      "\n",
      "Categories names:  ['C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23']\n"
     ]
    }
   ],
   "source": [
    "target_names = train_set.target_names\n",
    "categories = test_set.target_names\n",
    "\n",
    "print \"%d categories\" % len(categories)\n",
    "print\n",
    "print \"Categories names: \",categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of documents per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas and default dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoty id</th>\n",
       "      <th># of documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C19</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C18</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C13</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C12</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C11</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C10</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C17</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C16</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C15</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C14</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C22</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C23</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C20</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C21</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C08</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C09</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C01</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C02</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C03</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C04</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>C05</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>C06</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C07</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Categoty id  # of documents\n",
       "0          C19             191\n",
       "1          C18             388\n",
       "2          C13             281\n",
       "3          C12             491\n",
       "4          C11             162\n",
       "5          C10             621\n",
       "6          C17             295\n",
       "7          C16             200\n",
       "8          C15             215\n",
       "9          C14            1249\n",
       "10         C22              92\n",
       "11         C23            1799\n",
       "12         C20             525\n",
       "13         C21             546\n",
       "14         C08             473\n",
       "15         C09             125\n",
       "16         C01             423\n",
       "17         C02             158\n",
       "18         C03              65\n",
       "19         C04            1163\n",
       "20         C05             283\n",
       "21         C06             588\n",
       "22         C07             100"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "#iterate all the targeted files\n",
    "files_in_categories = defaultdict(int)\n",
    "for target in train_set.target:\n",
    "    files_in_categories[train_set.target_names[target]] += 1\n",
    "\n",
    "df = pd.DataFrame(files_in_categories.items(), columns=['Categoty id', '# of documents'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms distrebution category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding all the terms into 'term_per_category', lets sort it and print it's top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Category  C19\n",
      "  Categoty id  top terms\n",
      "0     patient       2279\n",
      "1      diabet       1133\n",
      "2     thyroid        814\n",
      "3         +/-        798\n",
      "4        less        664\n",
      "5       level        651\n",
      "6           p        587\n",
      "7       studi        572\n",
      "8      normal        508\n",
      "9       group        472\n",
      "================================================================================\n",
      "Category  C18\n",
      "  Categoty id  top terms\n",
      "0     patient       3062\n",
      "1      diabet       2764\n",
      "2         +/-       1875\n",
      "3     insulin       1785\n",
      "4        less       1481\n",
      "5       level       1448\n",
      "6     control       1382\n",
      "7           p       1354\n",
      "8       group       1342\n",
      "9       studi       1286\n",
      "================================================================================\n",
      "Category  C13\n",
      "  Categoty id  top terms\n",
      "0     patient       1913\n",
      "1       women       1769\n",
      "2    pregnanc       1388\n",
      "3         use        818\n",
      "4       fetal        786\n",
      "5       group        778\n",
      "6      infect        706\n",
      "7       studi        697\n",
      "8   treatment        664\n",
      "9         may        616\n",
      "================================================================================\n",
      "Category  C12\n",
      "  Categoty id  top terms\n",
      "0     patient       5666\n",
      "1       renal       3372\n",
      "2   treatment       1641\n",
      "3    patients       1379\n",
      "4       studi       1268\n",
      "5        cell       1243\n",
      "6     prostat       1209\n",
      "7         +/-       1191\n",
      "8       tumor       1184\n",
      "9         use       1183\n",
      "================================================================================\n",
      "Category  C11\n",
      "  Categoty id  top terms\n",
      "0     patient       1394\n",
      "1         eye        951\n",
      "2       retin        505\n",
      "3      visual        476\n",
      "4         use        388\n",
      "5         one        352\n",
      "6        case        344\n",
      "7       group        340\n",
      "8      ocular        339\n",
      "9    patients        326\n",
      "================================================================================\n",
      "Category  C10\n",
      "  Categoty id  top terms\n",
      "0     patient       7081\n",
      "1       group       1952\n",
      "2    patients       1920\n",
      "3       studi       1814\n",
      "4         use       1572\n",
      "5        pain       1515\n",
      "6         may       1469\n",
      "7   treatment       1393\n",
      "8        case       1347\n",
      "9        less       1315\n",
      "================================================================================\n",
      "Category  C17\n",
      "  Categoty id  top terms\n",
      "0     patient       2791\n",
      "1        skin       1090\n",
      "2        cell       1070\n",
      "3   treatment        775\n",
      "4       studi        713\n",
      "5    patients        684\n",
      "6         may        662\n",
      "7         use        628\n",
      "8      lesion        621\n",
      "9        case        594\n",
      "================================================================================\n",
      "Category  C16\n",
      "   Categoty id  top terms\n",
      "0      patient       1856\n",
      "1        group        667\n",
      "2       infant        623\n",
      "3        studi        550\n",
      "4          age        525\n",
      "5       arteri        517\n",
      "6         less        506\n",
      "7  ventricular        463\n",
      "8          use        448\n",
      "9          two        446\n",
      "================================================================================\n",
      "Category  C15\n",
      "  Categoty id  top terms\n",
      "0     patient       2873\n",
      "1        cell       1254\n",
      "2    patients        725\n",
      "3        case        628\n",
      "4       group        580\n",
      "5       studi        577\n",
      "6           p        555\n",
      "7   treatment        551\n",
      "8      anemia        544\n",
      "9        less        540\n",
      "================================================================================\n",
      "Category  C14\n",
      "   Categoty id  top terms\n",
      "0      patient      15844\n",
      "1          +/-       7038\n",
      "2         less       5485\n",
      "3            p       5443\n",
      "4        group       5375\n",
      "5       arteri       5174\n",
      "6  ventricular       4434\n",
      "7     coronari       4219\n",
      "8        blood       4051\n",
      "9        heart       4035\n",
      "================================================================================\n",
      "Category  C22\n",
      "  Categoty id  top terms\n",
      "0       model        534\n",
      "1         rat        474\n",
      "2        anim        375\n",
      "3       studi        372\n",
      "4      infect        347\n",
      "5       group        323\n",
      "6      effect        312\n",
      "7       human        309\n",
      "8         +/-        293\n",
      "9     control        286\n",
      "================================================================================\n",
      "Category  C23\n",
      "  Categoty id  top terms\n",
      "0     patient      20935\n",
      "1       group       5886\n",
      "2    patients       4880\n",
      "3        less       4801\n",
      "4       studi       4580\n",
      "5         use       4558\n",
      "6           p       4397\n",
      "7         +/-       4277\n",
      "8        case       3864\n",
      "9   treatment       3802\n",
      "================================================================================\n",
      "Category  C20\n",
      "  Categoty id  top terms\n",
      "0     patient       5935\n",
      "1        cell       2762\n",
      "2      infect       2419\n",
      "3         hiv       1886\n",
      "4       human       1765\n",
      "5    patients       1634\n",
      "6       studi       1552\n",
      "7         use       1440\n",
      "8         aid       1330\n",
      "9         may       1245\n",
      "================================================================================\n",
      "Category  C21\n",
      "  Categoty id  top terms\n",
      "0     patient       5073\n",
      "1      injuri       2556\n",
      "2         use       1815\n",
      "3     alcohol       1707\n",
      "4       studi       1463\n",
      "5    patients       1330\n",
      "6      trauma       1327\n",
      "7   treatment       1308\n",
      "8        case       1267\n",
      "9       group       1257\n",
      "================================================================================\n",
      "Category  C08\n",
      "  Categoty id  top terms\n",
      "0     patient       5405\n",
      "1        lung       2581\n",
      "2   pulmonari       1780\n",
      "3       group       1598\n",
      "4    patients       1463\n",
      "5        less       1440\n",
      "6           p       1375\n",
      "7       studi       1359\n",
      "8         +/-       1187\n",
      "9   treatment       1185\n",
      "================================================================================\n",
      "Category  C09\n",
      "  Categoty id  top terms\n",
      "0     patient        961\n",
      "1         ear        372\n",
      "2         use        352\n",
      "3        case        304\n",
      "4       group        297\n",
      "5   treatment        283\n",
      "6       studi        267\n",
      "7       nasal        266\n",
      "8   carcinoma        261\n",
      "9      result        233\n",
      "================================================================================\n",
      "Category  C01\n",
      "  Categoty id  top terms\n",
      "0     patient       4263\n",
      "1      infect       2727\n",
      "2   treatment       1251\n",
      "3       group       1232\n",
      "4        case       1126\n",
      "5    patients       1000\n",
      "6         use        976\n",
      "7        less        972\n",
      "8       studi        909\n",
      "9    children        904\n",
      "================================================================================\n",
      "Category  C02\n",
      "    Categoty id  top terms\n",
      "0       patient       1428\n",
      "1        infect       1316\n",
      "2          viru        847\n",
      "3         human        826\n",
      "4           hiv        656\n",
      "5          cell        573\n",
      "6  immunodefici        466\n",
      "7         studi        449\n",
      "8      patients        447\n",
      "9         group        434\n",
      "================================================================================\n",
      "Category  C03\n",
      "  Categoty id  top terms\n",
      "0     patient        422\n",
      "1      infect        377\n",
      "2     malaria        260\n",
      "3        case        176\n",
      "4   treatment        174\n",
      "5     parasit        168\n",
      "6       studi        159\n",
      "7         day        158\n",
      "8  falciparum        153\n",
      "9    antibodi        146\n",
      "================================================================================\n",
      "Category  C04\n",
      "  Categoty id  top terms\n",
      "0     patient      12965\n",
      "1        cell       7885\n",
      "2       tumor       7343\n",
      "3      cancer       5748\n",
      "4   carcinoma       4597\n",
      "5    patients       3359\n",
      "6        case       3335\n",
      "7   treatment       3154\n",
      "8       studi       3045\n",
      "9         use       2942\n",
      "================================================================================\n",
      "Category  C05\n",
      "  Categoty id  top terms\n",
      "0     patient       2768\n",
      "1        bone       1297\n",
      "2    patients        797\n",
      "3   treatment        721\n",
      "4         use        713\n",
      "5       studi        709\n",
      "6        case        709\n",
      "7       group        679\n",
      "8      result        651\n",
      "9         may        598\n",
      "================================================================================\n",
      "Category  C06\n",
      "  Categoty id  top terms\n",
      "0     patient       7900\n",
      "1    patients       1750\n",
      "2        less       1737\n",
      "3       studi       1634\n",
      "4           p       1570\n",
      "5       group       1556\n",
      "6       liver       1411\n",
      "7   treatment       1360\n",
      "8       ulcer       1238\n",
      "9         use       1207\n",
      "================================================================================\n",
      "Category  C07\n",
      "  Categoty id  top terms\n",
      "0     patient        712\n",
      "1        oral        315\n",
      "2        case        313\n",
      "3   treatment        305\n",
      "4        cell        267\n",
      "5         use        214\n",
      "6       gland        204\n",
      "7     present        200\n",
      "8       studi        193\n",
      "9      lesion        192\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "for dic in term_per_category:\n",
    "    sorted_x = sorted(term_per_category[dic].items(), key=lambda kv: kv[1] , reverse=True)\n",
    "    df = pd.DataFrame(sorted_x[:10], columns=['Categoty id', 'top terms']) \n",
    "    print '=' * 80\n",
    "    print 'Category ', dic\n",
    "    print df \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the lists of top 10 frequent words in each category we can see that there are words\n",
    "that appeare many times in mane categories.\n",
    "We can remove these words and gain more accurecy.\n",
    "In PART 2 of the assignemt we will remove there words from our corpus.\n",
    "The frequent words that apprear in many categoried and we will remove are:\n",
    "- patient\n",
    "- infect\n",
    "- case\n",
    "- use\n",
    "- study / studies\n",
    "- cell\n",
    "- treatment\n",
    "- increase\n",
    "- '+/-'\n",
    "- group\n",
    "- p \n",
    "- result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the expected challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenges we expect to meet are:\n",
    "- Terms that are common in all categories may cause mistakes in fitting the models.\n",
    "- Dealing with technical issues such as getting to know python and the different libraries\n",
    "- Improving the algorithm in order to increase accuracy may be complicated, there might be parameters we didn't learn how to handle, and that can affect on our results.\n",
    "- Exploring and checking what are the best algoritms for problem of this kind (classification of short documents).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center style=\"color:#1E90FF\">\n",
    "    <h1>\n",
    "2. Document classification\n",
    "    </h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new stop words list according to the findings from part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extended_stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "extended_stop_words += ['patient', 'infect', 'case' , 'use', 'study','studies', 'cell', 'treatment', 'increase','+/-', 'group', 'p','result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We chose to clasify the documents by using Tf-Idf and Ngram as features Extraction Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features from the training data using TF-IDF - Since we used 'TfidfVectorizer' it combines all the options of CountVectorizer and TfidfTransformer in a single model, so these are 2 feature extraction in one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=extended_stop_words)\n",
    "feature_extraction_train = vectorizer.fit_transform(train_set.data)\n",
    "print (feature_extraction_train.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use features extraction from the test data using the same vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "feature_extraction_test = vectorizer.transform(test_set.data)\n",
    "\n",
    "print (feature_extraction_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following methode we go over the given algorithm 'clf', classifying, and saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_display = []\n",
    "\n",
    "def benchmark(clf, name):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    \n",
    "    clf.fit(feature_extraction_train, train_set.target)\n",
    "\n",
    "    pred = clf.predict(feature_extraction_test)\n",
    "\n",
    "    score = metrics.accuracy_score(test_set.target, pred)\n",
    "    result_display.append([name, 'Naive run', score])\n",
    "    print ('accuracy: %f' %score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "accuracy: 0.252258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "benchmark(MultinomialNB(), 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "accuracy: 0.398335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "benchmark(KNeighborsClassifier(), 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "accuracy: 0.445221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "benchmark(SGDClassifier(), 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion:</h2>\n",
    "<p>\n",
    "We can see that the best accuracy achived by the SVM - SGDClassifier algorithm\n",
    "</p>\n",
    "<h2>Try to Improve Accurecy:</h2>\n",
    "<p>\n",
    "We will try to improve it using Pipeline and tune the algorithm and feature extraction parameters:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nb_clf = Pipeline([('vect', vectorizer),('clf', MultinomialNB())])\n",
    "knn_clf = Pipeline([('vect', vectorizer),('clf', KNeighborsClassifier())])\n",
    "sgd_clf = Pipeline([('vect', vectorizer),('clf', SGDClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the best parameters for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive biase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.26924182881242215)\n",
      "('Best params: ', {'vect__ngram_range': (1, 1), 'vect__max_df': 0.6, 'clf__fit_prior': False, 'clf__alpha': 0.01})\n"
     ]
    }
   ],
   "source": [
    "parameters =  {'vect__max_df': (0.3,0.4,0.5,0.6,0.7, 0.75, 1.0),\n",
    "               'vect__ngram_range': ((1, 1), (1, 2)),# unigrams or bigrams\n",
    "               'clf__alpha': (0.0001, 0.01,1.0), \n",
    "               'clf__fit_prior':[True, False] }\n",
    "\n",
    "naive_clf = GridSearchCV(nb_clf, parameters)\n",
    "naive_clf = naive_clf.fit(train_set.data, train_set.target)\n",
    "\n",
    "print('Best score: ',naive_clf.best_score_)\n",
    "print('Best params: ',naive_clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in the folowing photo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![naive biase results](./images/nb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best score: ', 0.36068244991852777)\n",
      "('Best params: ', {'vect__ngram_range': (1, 2), 'clf__algorithm': 'auto', 'vect__max_df': 0.6, 'clf__leaf_size': 10, 'clf__weights': 'uniform', 'clf__n_neighbors': 7})\n"
     ]
    }
   ],
   "source": [
    "knn_parameters = {'vect__max_df': (0.3,0.4,0.5,0.6,0.7, 0.75, 1.0),\n",
    "               'vect__ngram_range': ((1, 1), (1, 2)),# unigrams or bigrams\n",
    "               'clf__algorithm': ('auto', 'ball_tree', 'kd_tree', 'brute'), \n",
    "               'clf__leaf_size': (10,20,30,40),\n",
    "               'clf__n_neighbors': (2,5,7),\n",
    "               'clf__weights': ['uniform', 'distance']}\n",
    "\n",
    "knn_gs = GridSearchCV(knn_clf, knn_parameters)\n",
    "knn_gs = knn_gs.fit(train_set.data, train_set.target)\n",
    "print('Best score: ',knn_gs.best_score_)\n",
    "print('Best params: ',knn_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is shown in the following photo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn results](./images/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters =  {'vect__max_df': (0.3,0.4,0.5,0.6,0.7, 0.75, 1.0),\n",
    "               'vect__ngram_range': ((1, 1), (1, 2)),# unigrams or bigrams\n",
    "               'clf__alpha': (0.0001, 0.01,1.0), \n",
    "               'clf__loss':['hinge','epsilon_insensitive','modified_huber'],\n",
    "               'clf__penalty': ('l2', 'elasticnet'),\n",
    "               'clf__average':[False, True]}\n",
    "\n",
    "gs_clf = GridSearchCV(sgd_clf, parameters)\n",
    "gs_clf = gs_clf.fit(train_set.data, train_set.target)\n",
    "print('Best score: ',gs_clf.best_score_)\n",
    "print('Best params: ',gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shown in a photo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm results](./images/svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive biase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved accuracy:   0.406\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (1,1), sublinear_tf=True, max_df=0.6, stop_words=extended_stop_words)\n",
    "\n",
    "improved_features_train = vectorizer.fit_transform(train_set.data)\n",
    "improved_features_test = vectorizer.transform(test_set.data)\n",
    "\n",
    "clf = MultinomialNB(fit_prior=False,  alpha = 0.01)\n",
    "\n",
    "clf.fit(improved_features_train, train_set.target)\n",
    "pred = clf.predict(improved_features_test)\n",
    "\n",
    "score = metrics.accuracy_score(test_set.target, pred)\n",
    "result_display.append(['Naive Bayes', 'Improved', score])\n",
    "\n",
    "print(\"improved accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved accuracy:   0.416\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (1,2), sublinear_tf=True, max_df=0.6, stop_words=extended_stop_words)\n",
    "\n",
    "improved_features_train = vectorizer.fit_transform(train_set.data)\n",
    "improved_features_test = vectorizer.transform(test_set.data)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=7, weights='uniform', algorithm='auto',leaf_size=10)\n",
    "\n",
    "clf.fit(improved_features_train, train_set.target)\n",
    "pred = clf.predict(improved_features_test)\n",
    "\n",
    "score = metrics.accuracy_score(test_set.target, pred)\n",
    "result_display.append(['KNN', 'Improved', score])\n",
    "\n",
    "print(\"improved accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved accuracy:   0.218\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (1,2), sublinear_tf=True, max_df=0.7, stop_words=extended_stop_words)\n",
    "\n",
    "improved_features_train = vectorizer.fit_transform(train_set.data)\n",
    "improved_features_test = vectorizer.transform(test_set.data)\n",
    "\n",
    "clf = SGDClassifier(average=True, loss=\"modified_huber\", penalty = 'elasticnet', alpha = 0.0001)\n",
    "\n",
    "clf.fit(improved_features_train, train_set.target)\n",
    "pred = clf.predict(improved_features_test)\n",
    "\n",
    "score = metrics.accuracy_score(test_set.target, pred)\n",
    "result_display.append(['SVM', 'Improved', score])\n",
    "\n",
    "print(\"improved accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's display the results and analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm name</th>\n",
       "      <th>Type of run</th>\n",
       "      <th>Accuracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Naive run</td>\n",
       "      <td>0.252258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive run</td>\n",
       "      <td>0.398335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Naive run</td>\n",
       "      <td>0.445221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Improved</td>\n",
       "      <td>0.406346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Improved</td>\n",
       "      <td>0.416477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Improved</td>\n",
       "      <td>0.217938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm name Type of run  Accuracy score\n",
       "0    Naive Bayes   Naive run        0.252258\n",
       "1            KNN   Naive run        0.398335\n",
       "2            SVM   Naive run        0.445221\n",
       "3    Naive Bayes    Improved        0.406346\n",
       "4            KNN    Improved        0.416477\n",
       "5            SVM    Improved        0.217938"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result_display, columns=['Algorithm name', 'Type of run', 'Accuracy score'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the SVM model has returned the best result, and that was even before using the improved parameters.\n",
    "The meaning is that even after trying to tune the different parameters, the 'Naive run' for SVM has returned the best result in 0.443 accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy in all the algorithms is not the as high as we accepted to receive. Since it took a lot of time to use the GridSearch, it was very disappointing to see the accuracy is lower then expected. We assume the reason is that the result we got was because the parameters were overfitting to the train set, and therefore in the real-time test the prediction to the test set wasn't good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task challenges and effective solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finding the parameters to improve the algorithms took a lot of time \n",
    "* Arrange all the information we found in a nice way\n",
    "* The results of prediction with the tuned parameters were very low\n",
    "* Understand the best way to avoid 'Overfitting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effective solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The train set should include a variaty if information so that 'Overfitting' will be prevented\n",
    "* Use strong and multiple computers to avoid wating time on computing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
